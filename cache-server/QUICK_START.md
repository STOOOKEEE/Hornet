# âš¡ Quick Start Guide

## ğŸš€ DÃ©marrage en 5 minutes

### 1. Installation

```bash
cd cache-server
npm install
```

### 2. Configuration

```bash
cp .env.example .env
```

Ã‰diter `.env`:
```bash
GEMINI_API_KEY=votre_cle_ici
```

### 3. DÃ©marrer Redis

**macOS:**
```bash
brew install redis
brew services start redis
```

**Linux:**
```bash
sudo apt install redis-server
sudo systemctl start redis
```

**Docker:**
```bash
docker run -d -p 6379:6379 redis:7-alpine
```

### 4. DÃ©marrer le serveur

```bash
npm start
```

### 5. Tester

```bash
# Health check
curl http://localhost:3001/api/health

# Attendre 2 minutes pour le premier refresh, ou trigger manuellement:
curl -X POST http://localhost:3001/api/refresh

# Get analysis
curl http://localhost:3001/api/analysis
```

---

## ğŸ³ DÃ©marrage avec Docker (le plus simple)

```bash
cd cache-server
cp .env.example .env
# Ã‰diter .env avec votre clÃ© Gemini

docker-compose up -d
```

C'est tout ! Le serveur et Redis sont prÃªts.

---

## ğŸ“Š VÃ©rifier que Ã§a fonctionne

### 1. Health check

```bash
curl http://localhost:3001/api/health
```

Devrait retourner:
```json
{
  "success": true,
  "healthy": true,
  "cache": { "healthy": true },
  "redis": { "connected": true },
  "scheduler": { "isRunning": true }
}
```

### 2. Trigger un refresh

```bash
curl -X POST http://localhost:3001/api/refresh
```

Attendre ~10 secondes (temps de fetch DeFiLlama + Gemini).

### 3. Get l'analyse

```bash
curl http://localhost:3001/api/analysis | jq
```

Devrait retourner les stratÃ©gies AI.

---

## ğŸ”— IntÃ©grer avec votre frontend

### Option 1: Modifier l'API route existante

Remplacer `/pages/api/ai/analyze-strategies.ts`:

```typescript
import type { NextApiRequest, NextApiResponse } from 'next';

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  if (req.method !== 'GET') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    // Appeler le cache server au lieu de faire les requÃªtes
    const response = await fetch('http://localhost:3001/api/analysis');
    const data = await response.json();

    if (!data.success) {
      throw new Error(data.error || 'Failed to fetch analysis');
    }

    // Retourner le mÃªme format que avant
    res.status(200).json({
      success: true,
      strategies: data.data.strategies,
      summary: data.data.summary,
      marketInsights: data.data.marketInsights || '',
      warnings: data.data.warnings,
      totalPoolsAnalyzed: data.data.totalPoolsAnalyzed,
      totalPoolsAvailable: data.metadata.totalPools,
    });
  } catch (error) {
    console.error('Error fetching from cache server:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to fetch analysis',
      message: error instanceof Error ? error.message : 'Unknown error',
    });
  }
}
```

### Option 2: Appeler directement depuis le frontend

Modifier `/components/dashboard/AIOptimizer.tsx`:

```typescript
const handleSearch = async () => {
  try {
    setIsSearching(true);
    setError(null);

    // Appeler directement le cache server
    const response = await fetch('http://localhost:3001/api/analysis');
    
    if (!response.ok) {
      throw new Error('Failed to fetch analysis');
    }

    const data = await response.json();
    
    if (!data.success) {
      throw new Error(data.message || 'Analysis failed');
    }

    // Adapter le format si nÃ©cessaire
    setAnalysis({
      strategies: data.data.strategies,
      summary: data.data.summary,
      marketInsights: data.data.marketInsights || '',
      warnings: data.data.warnings,
      totalPoolsAnalyzed: data.data.totalPoolsAnalyzed,
    });
  } catch (err) {
    console.error('Search error:', err);
    setError(err instanceof Error ? err.message : 'An error occurred');
  } finally {
    setIsSearching(false);
  }
};
```

---

## ğŸŒ DÃ©ployer en production

### Railway (recommandÃ©, gratuit)

```bash
# Installer Railway CLI
npm install -g @railway/cli

# Login
railway login

# Deploy
cd cache-server
railway init
railway add redis
railway up

# Configurer les variables d'environnement dans le dashboard
```

### Render (gratuit aussi)

1. Push votre code sur GitHub
2. Aller sur render.com
3. New > Web Service
4. Connecter votre repo
5. Ajouter Redis service
6. Configurer les variables d'environnement

### VPS (DigitalOcean, Linode, etc.)

```bash
# Sur votre VPS
git clone <your-repo>
cd cache-server
npm install

# Installer PM2
npm install -g pm2

# DÃ©marrer
npm run pm2:start

# Configurer pour dÃ©marrer au boot
pm2 startup
pm2 save
```

---

## ğŸ“ˆ Monitoring

### Logs en temps rÃ©el

```bash
# Avec PM2
pm2 logs hornet-cache

# Avec Docker
docker-compose logs -f cache-server
```

### MÃ©triques

```bash
# Stats du serveur
curl http://localhost:3001/api/stats | jq

# Metadata du cache
curl http://localhost:3001/api/metadata | jq
```

---

## ğŸ”§ Configuration avancÃ©e

### Changer l'intervalle de refresh

Dans `.env`:
```bash
REFRESH_INTERVAL_MINUTES=5  # Refresh toutes les 5 minutes
```

### Augmenter le nombre de pools analysÃ©s

Dans `src/config/index.js`:
```javascript
analysis: {
  maxPoolsToAnalyze: 200,  // Analyser 200 pools au lieu de 100
}
```

### Activer le mode debug

Dans `.env`:
```bash
LOG_LEVEL=debug
LOG_PRETTY=true
```

---

## â“ FAQ

### Q: Combien coÃ»te le serveur ?

**R:** Gratuit sur Railway/Render (avec limites), ou ~$5-10/mois sur un VPS.

### Q: Combien coÃ»te l'API Gemini ?

**R:** Avec un refresh toutes les 2 minutes:
- 30 requÃªtes/heure
- 720 requÃªtes/jour
- ~$0.50-1/jour selon le modÃ¨le

Vs sans cache: ~$50-100/jour si 100 utilisateurs.

### Q: Peut-on utiliser plusieurs chaÃ®nes ?

**R:** Oui, modifier `src/config/index.js`:
```javascript
analysis: {
  chains: ['Base', 'Ethereum', 'Arbitrum'],
}
```

### Q: Comment ajouter un webhook ?

**R:** Dans `.env`:
```bash
WEBHOOK_URL=https://your-webhook.com
WEBHOOK_ON_ERROR=true
WEBHOOK_ON_SUCCESS=false
```

---

## ğŸ‰ C'est tout !

Votre serveur de cache est prÃªt. Les utilisateurs auront maintenant des rÃ©ponses instantanÃ©es au lieu d'attendre 8-12 secondes.

**Ã‰conomies:**
- âš¡ 99% plus rapide (< 50ms vs 8-12s)
- ğŸ’° 99% moins cher (1 requÃªte/2min vs 1 requÃªte/utilisateur)
- ğŸ¯ Meilleure UX (pas d'attente)
